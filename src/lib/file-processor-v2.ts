import fs from 'fs/promises'
import path from 'path'
import crypto from 'crypto'
import { TextSplitter } from './text-splitter'
import { FILE_CONFIG } from './file-config'
import {
  processDocumentFile,
  documentProcessorFactory,
} from './document-processors'

export interface ProcessedFileV2 {
  text: string
  chunks: Array<{
    content: string
    index: number
  }>
  metadata: {
    format: string
    processor: string
    originalSize: number
    pages?: number
    title?: string
    author?: string
    subject?: string
    keywords?: string[]
  }
  hash: string
}

/**
 * Enhanced File Processor with Multi-Format Support
 * Supports PDF, TXT, FB2, EPUB, DOC, DOCX
 */
export class MultiFormatFileProcessor {
  constructor() {
    // TextSplitter –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±–µ—Ä–µ—Ç—Å—è –∏–∑ FILE_CONFIG
  }

  /**
   * Process any supported file format
   * @param filePath - Path to the file
   * @param fileName - Original file name
   * @param mimeType - MIME type (optional)
   * @returns ProcessedFileV2 with text, chunks, and metadata
   */
  async processFile(
    filePath: string,
    fileName: string,
    mimeType?: string
  ): Promise<ProcessedFileV2> {
    try {
      console.log(`üîÑ Processing file: ${fileName}`)

      // Check if file format is supported
      if (!documentProcessorFactory.isSupported(fileName, mimeType)) {
        const supportedFormats =
          documentProcessorFactory.getSupportedExtensions()
        throw new Error(
          `Unsupported file format: ${fileName}. Supported formats: ${supportedFormats.join(
            ', '
          )}`
        )
      }

      // Check if file exists and is accessible
      try {
        await fs.access(filePath)
        console.log(`‚úÖ File accessible: ${fileName}`)
      } catch (accessError) {
        throw new Error(`File not found or inaccessible: ${filePath}`)
      }

      // Read file buffer
      const buffer = await fs.readFile(filePath)
      console.log(`üìñ File read: ${buffer.length} bytes`)

      // Calculate file hash
      const hash = this.calculateFileHash(buffer)
      console.log(`üîê File hash: ${hash}`)

      // Process document based on format
      const { text, processor, format } = await processDocumentFile(
        filePath,
        buffer,
        fileName,
        mimeType
      )

      console.log(
        `‚úÖ Text extracted: ${text.length} characters using ${processor}`
      )

      if (!text.trim()) {
        throw new Error(`No text content extracted from file: ${fileName}`)
      }

      // Create chunks using smart chunking strategy
      const chunks = await this.createSmartChunks(text, format)
      console.log(`üìù Created ${chunks.length} chunks`)

      // Build metadata
      const metadata = {
        format,
        processor,
        originalSize: buffer.length,
        title: this.extractTitle(text, format),
        // Add more metadata extraction based on format
        ...(format === 'pdf' && { pages: this.estimatePages(text) }),
      }

      return {
        text,
        chunks,
        metadata,
        hash,
      }
    } catch (error) {
      console.error(`‚ùå Error processing file ${fileName}:`, error)
      throw new Error(`File processing failed: ${error.message}`)
    }
  }

  /**
   * Smart chunking strategy based on file format
   * Different strategies for different document types
   */
  private async createSmartChunks(
    text: string,
    format: string
  ): Promise<Array<{ content: string; index: number }>> {
    switch (format) {
      case 'fb2':
      case 'epub':
        // For books, try to split by chapters/sections
        return this.chunkByChapters(text)

      case 'docx':
      case 'doc':
        // For documents, split by paragraphs
        return this.chunkByParagraphs(text)

      case 'pdf':
      case 'txt':
      default:
        // Standard splitting for PDF and TXT
        return this.chunkByTokens(text)
    }
  }

  /**
   * Split text by chapters for books (FB2, EPUB) - MEMORY OPTIMIZED
   */
  private chunkByChapters(
    text: string
  ): Array<{ content: string; index: number }> {
    // AGGRESSIVE MEMORY PROTECTION: Hard limits to prevent crashes
    const MAX_FILE_SIZE = FILE_CONFIG.MAX_TEXT_LENGTH // Use config limit
    const MAX_CHUNKS = FILE_CONFIG.MAX_CHUNKS_PER_FILE // Use config limit

    if (text.length > MAX_FILE_SIZE) {
      console.warn(
        `üö® CRITICAL: File too large (${text.length} chars), truncating to ${MAX_FILE_SIZE} to prevent memory crash`
      )
      text =
        text.substring(0, MAX_FILE_SIZE) +
        '\n\n[... —Ñ–∞–π–ª –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã ...]'
    }

    // Use token-based chunking for all large files
    if (text.length > 10000) {
      console.log(`üìù Large file detected, using token-based chunking`)
      return this.chunkByTokens(text)
    }

    // Look for chapter markers with memory-efficient approach
    const chapterMarkers = [
      /\n\s*–ì–ª–∞–≤–∞\s+\d+/gi,
      /\n\s*–ì–õ–ê–í–ê\s+\d+/gi,
      /\n\s*Chapter\s+\d+/gi,
      /\n\s*\d+\.\s*\n/gm,
    ]

    let chapters = [text]

    // Try to split by chapter markers (memory optimized)
    for (const marker of chapterMarkers) {
      if (chapters.length >= MAX_CHUNKS) {
        console.warn(
          `‚ö†Ô∏è Too many chapters (${chapters.length}), stopping split`
        )
        break
      }

      const newChapters = []
      for (const chapter of chapters) {
        // More efficient splitting with limits
        const matches = Array.from(chapter.matchAll(marker))
        if (matches.length > 0 && matches.length < 5) {
          // Reduced limit
          // Limit matches
          let lastIndex = 0
          for (const match of matches) {
            if (newChapters.length >= MAX_CHUNKS) break

            // Add text before the match
            if (match.index > lastIndex) {
              const beforeText = chapter.substring(lastIndex, match.index)
              if (beforeText.trim().length > 100) {
                newChapters.push(beforeText)
              }
            }
            lastIndex = match.index
          }
          // Add remaining text after last match
          const remainingText = chapter.substring(lastIndex)
          if (
            remainingText.trim().length > 100 &&
            newChapters.length < MAX_CHUNKS
          ) {
            newChapters.push(remainingText)
          }
        } else {
          newChapters.push(chapter)
        }
      }
      chapters = newChapters.filter((c) => c.trim().length > 100)
    }

    // Final chunking with strict limits
    const finalChunks = []
    let chunkIndex = 0

    for (const chapter of chapters) {
      if (finalChunks.length >= MAX_CHUNKS) {
        console.warn(`‚ö†Ô∏è Reached maximum chunks limit (${MAX_CHUNKS})`)
        break
      }

      if (chapter.length > FILE_CONFIG.chunkSize * 2) {
        // Large chapter - use token splitting
        const tokenChunks = this.chunkByTokens(chapter)
        for (const chunk of tokenChunks) {
          if (finalChunks.length >= MAX_CHUNKS) break
          finalChunks.push({
            content: chunk.content,
            index: chunkIndex++,
          })
        }
      } else {
        finalChunks.push({
          content: chapter,
          index: chunkIndex++,
        })
      }
    }

    console.log(`üìù Created ${finalChunks.length} chunks from chapters`)
    return finalChunks
  }

  /**
   * Split text by paragraphs for documents (DOCX, DOC)
   */
  private chunkByParagraphs(
    text: string
  ): Array<{ content: string; index: number }> {
    const paragraphs = text.split(/\n\s*\n/).filter((p) => p.trim().length > 50) // Minimum 50 chars per paragraph

    const chunks = []
    let currentChunk = ''

    for (const paragraph of paragraphs) {
      if (currentChunk.length + paragraph.length > FILE_CONFIG.chunkSize) {
        if (currentChunk.trim()) {
          chunks.push({
            content: currentChunk.trim(),
            index: chunks.length,
          })
        }
        currentChunk = paragraph
      } else {
        currentChunk += (currentChunk ? '\n\n' : '') + paragraph
      }
    }

    // Add final chunk
    if (currentChunk.trim()) {
      chunks.push({
        content: currentChunk.trim(),
        index: chunks.length,
      })
    }

    return chunks.length > 0 ? chunks : this.chunkByTokens(text)
  }

  /**
   * Standard token-based splitting for PDF and TXT - MEMORY PROTECTED
   */
  private chunkByTokens(
    text: string
  ): Array<{ content: string; index: number }> {
    // Memory protection: limit text size
    if (text.length > FILE_CONFIG.MAX_TEXT_LENGTH) {
      console.warn(
        `üö® CRITICAL: Text too large (${text.length} chars), truncating to ${FILE_CONFIG.MAX_TEXT_LENGTH}`
      )
      text =
        text.substring(0, FILE_CONFIG.MAX_TEXT_LENGTH) +
        '\n\n[... —Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã ...]'
    }

    const chunks = TextSplitter.splitText(text)

    // Limit number of chunks to prevent memory issues
    if (chunks.length > FILE_CONFIG.MAX_CHUNKS_PER_FILE) {
      console.warn(
        `‚ö†Ô∏è Too many chunks (${chunks.length}), limiting to ${FILE_CONFIG.MAX_CHUNKS_PER_FILE}`
      )
      return chunks.slice(0, FILE_CONFIG.MAX_CHUNKS_PER_FILE).map((chunk) => ({
        content: chunk.content.trim(),
        index: chunk.index,
      }))
    }

    return chunks.map((chunk) => ({
      content: chunk.content.trim(),
      index: chunk.index,
    }))
  }

  /**
   * Calculate SHA-256 hash of file content
   */
  private calculateFileHash(buffer: Buffer): string {
    return crypto.createHash('sha256').update(buffer).digest('hex')
  }

  /**
   * Extract title from document text based on format
   */
  private extractTitle(text: string, format: string): string | undefined {
    const lines = text.split('\n').filter((line) => line.trim())

    if (lines.length === 0) return undefined

    // For books, first non-empty line is often the title
    if (format === 'fb2' || format === 'epub') {
      // Look for "–ù–∞–∑–≤–∞–Ω–∏–µ:" pattern first
      for (const line of lines.slice(0, 10)) {
        if (line.includes('–ù–∞–∑–≤–∞–Ω–∏–µ:')) {
          return line.replace('–ù–∞–∑–≤–∞–Ω–∏–µ:', '').trim()
        }
      }
      // Fallback to first substantial line
      return lines.find((line) => line.length > 5 && line.length < 100)
    }

    // For documents, first line is usually title
    if (format === 'docx' || format === 'doc') {
      return lines[0]?.length < 100 ? lines[0] : undefined
    }

    // For PDF and TXT, try to find a title-like line
    for (const line of lines.slice(0, 5)) {
      if (line.length > 5 && line.length < 100 && !line.includes('.')) {
        return line
      }
    }

    return undefined
  }

  /**
   * Estimate number of pages for PDF-like content
   */
  private estimatePages(text: string): number {
    // Rough estimate: 500 words per page
    const words = text.split(/\s+/).length
    return Math.ceil(words / 500)
  }

  /**
   * Get supported file formats
   */
  static getSupportedFormats(): {
    extensions: string[]
    mimeTypes: string[]
  } {
    return {
      extensions: documentProcessorFactory.getSupportedExtensions(),
      mimeTypes: documentProcessorFactory.getSupportedMimeTypes(),
    }
  }

  /**
   * Check if file format is supported
   */
  static isFormatSupported(fileName: string, mimeType?: string): boolean {
    return documentProcessorFactory.isSupported(fileName, mimeType)
  }
}

// Export singleton instance
export const multiFormatProcessor = new MultiFormatFileProcessor()

/**
 * Helper function to process file with error handling
 */
export async function processMultiFormatFile(
  filePath: string,
  fileName: string,
  mimeType?: string
): Promise<ProcessedFileV2> {
  return await multiFormatProcessor.processFile(filePath, fileName, mimeType)
}
