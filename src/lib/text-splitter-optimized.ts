import { FILE_CONFIG } from './file-config'

export interface TextChunk {
  content: string
  index: number
  start: number
  end: number
  tokenCount?: number
}

export interface ChunkingOptions {
  chunkSize?: number // –≤ —Ç–æ–∫–µ–Ω–∞—Ö
  chunkOverlap?: number // –≤ —Ç–æ–∫–µ–Ω–∞—Ö
  preserveStructure?: boolean // —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π/–∞–±–∑–∞—Ü–µ–≤
}

export class OptimizedTextSplitter {
  /**
   * –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ (fallback –±–µ–∑ tiktoken)
   * –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —ç–º–ø–∏—Ä–∏—á–µ—Å–∫—É—é —Ñ–æ—Ä–º—É–ª—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤
   */
  static countTokens(text: string): number {
    // –ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤
    // –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ OpenAI tokenizer

    // –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã —Å–∏–º–≤–æ–ª–æ–≤
    const russianChars = (text.match(/[–∞-—è—ë]/gi) || []).length
    const englishWords = (text.match(/\b[a-z]+\b/gi) || []).length
    const numbers = (text.match(/\d+/g) || []).length
    const punctuation = (
      text.match(/[.,!?;:()[\]{}"'`~@#$%^&*+=<>\/\\|_-]/g) || []
    ).length
    const spaces = (text.match(/\s/g) || []).length

    // –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    let estimatedTokens = 0

    // –†—É—Å—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã: ~2.5 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
    estimatedTokens += russianChars / 2.5

    // –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å–ª–æ–≤–∞: ~0.75 —Ç–æ–∫–µ–Ω–∞ –Ω–∞ —Å–ª–æ–≤–æ
    estimatedTokens += englishWords * 0.75

    // –ß–∏—Å–ª–∞: –æ–±—ã—á–Ω–æ 1 —Ç–æ–∫–µ–Ω –Ω–∞ —á–∏—Å–ª–æ
    estimatedTokens += numbers

    // –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è: ~0.5 —Ç–æ–∫–µ–Ω–∞ –Ω–∞ —Å–∏–º–≤–æ–ª
    estimatedTokens += punctuation * 0.5

    // –ü—Ä–æ–±–µ–ª—ã –æ–±—ã—á–Ω–æ –Ω–µ —Å—á–∏—Ç–∞—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏

    // –ú–∏–Ω–∏–º—É–º 1 —Ç–æ–∫–µ–Ω –¥–ª—è –Ω–µ–ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    return Math.max(1, Math.ceil(estimatedTokens))
  }

  /**
   * –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
   * –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö OpenAI –¥–ª—è embeddings
   */
  static splitTextOptimized(
    text: string,
    options: ChunkingOptions = {}
  ): TextChunk[] {
    const {
      chunkSize = 1000, // —Ç–æ–∫–µ–Ω–æ–≤ - —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è OpenAI
      chunkOverlap = 200, // —Ç–æ–∫–µ–Ω–æ–≤ - 20% –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–∞
      preserveStructure = true,
    } = options

    // Memory protection
    if (text.length > FILE_CONFIG.MAX_TEXT_LENGTH) {
      console.warn(
        `üö® CRITICAL: Text too large (${text.length} chars), truncating to ${FILE_CONFIG.MAX_TEXT_LENGTH}`
      )
      text =
        text.substring(0, FILE_CONFIG.MAX_TEXT_LENGTH) +
        '\n\n[... —Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã ...]'
    }

    const chunks: TextChunk[] = []

    if (preserveStructure) {
      return this.splitWithStructurePreservation(text, chunkSize, chunkOverlap)
    } else {
      return this.splitByTokens(text, chunkSize, chunkOverlap)
    }
  }

  /**
   * –†–∞–∑–±–∏–≤–∫–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∞–±–∑–∞—Ü—ã)
   */
  private static splitWithStructurePreservation(
    text: string,
    chunkSize: number,
    chunkOverlap: number
  ): TextChunk[] {
    const chunks: TextChunk[] = []

    // –°–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã
    const paragraphs = text.split(/\n\s*\n/).filter((p) => p.trim().length > 0)

    let currentChunk = ''
    let currentTokens = 0
    let start = 0
    let index = 0

    for (const paragraph of paragraphs) {
      const paragraphTokens = this.countTokens(paragraph)

      // –ï—Å–ª–∏ –∞–±–∑–∞—Ü —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
      if (paragraphTokens > chunkSize) {
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫ –µ—Å–ª–∏ –µ—Å—Ç—å
        if (currentChunk.trim()) {
          chunks.push({
            content: currentChunk.trim(),
            index,
            start,
            end: start + currentChunk.length,
            tokenCount: currentTokens,
          })
          index++
        }

        // –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–æ–π –∞–±–∑–∞—Ü
        const sentenceChunks = this.splitLargeParagraph(
          paragraph,
          chunkSize,
          chunkOverlap
        )
        sentenceChunks.forEach((chunk) => {
          chunks.push({
            ...chunk,
            index: index++,
            start: start + chunk.start,
            end: start + chunk.end,
          })
        })

        start += paragraph.length + 2 // +2 –¥–ª—è \n\n
        currentChunk = ''
        currentTokens = 0
        continue
      }

      // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–º–µ—Å—Ç–∏—Ç—Å—è –ª–∏ –∞–±–∑–∞—Ü –≤ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
      if (currentTokens + paragraphTokens <= chunkSize) {
        currentChunk += (currentChunk ? '\n\n' : '') + paragraph
        currentTokens += paragraphTokens
      } else {
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
        if (currentChunk.trim()) {
          chunks.push({
            content: currentChunk.trim(),
            index,
            start,
            end: start + currentChunk.length,
            tokenCount: currentTokens,
          })
          index++
        }

        // –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫
        currentChunk = paragraph
        currentTokens = paragraphTokens
        start += currentChunk.length + 2
      }
    }

    // –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
    if (currentChunk.trim()) {
      chunks.push({
        content: currentChunk.trim(),
        index,
        start,
        end: start + currentChunk.length,
        tokenCount: currentTokens,
      })
    }

    console.log(
      `üìù OptimizedTextSplitter created ${
        chunks.length
      } chunks with average ${Math.round(
        chunks.reduce((sum, c) => sum + (c.tokenCount || 0), 0) / chunks.length
      )} tokens per chunk`
    )

    return chunks
  }

  /**
   * –†–∞–∑–±–∏–≤–∫–∞ –±–æ–ª—å—à–æ–≥–æ –∞–±–∑–∞—Ü–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
   */
  private static splitLargeParagraph(
    paragraph: string,
    chunkSize: number,
    chunkOverlap: number
  ): TextChunk[] {
    const chunks: TextChunk[] = []
    const sentences = paragraph
      .split(/[.!?]+/)
      .filter((s) => s.trim().length > 0)

    let currentChunk = ''
    let currentTokens = 0
    let start = 0
    let index = 0

    for (const sentence of sentences) {
      const sentenceWithPunct = sentence.trim() + '.'
      const sentenceTokens = this.countTokens(sentenceWithPunct)

      if (currentTokens + sentenceTokens <= chunkSize) {
        currentChunk += (currentChunk ? ' ' : '') + sentenceWithPunct
        currentTokens += sentenceTokens
      } else {
        if (currentChunk.trim()) {
          chunks.push({
            content: currentChunk.trim(),
            index,
            start,
            end: start + currentChunk.length,
            tokenCount: currentTokens,
          })
          index++
        }

        currentChunk = sentenceWithPunct
        currentTokens = sentenceTokens
        start += currentChunk.length
      }
    }

    if (currentChunk.trim()) {
      chunks.push({
        content: currentChunk.trim(),
        index,
        start,
        end: start + currentChunk.length,
        tokenCount: currentTokens,
      })
    }

    return chunks
  }

  /**
   * –ü—Ä–æ—Å—Ç–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
   */
  private static splitByTokens(
    text: string,
    chunkSize: number,
    chunkOverlap: number
  ): TextChunk[] {
    const chunks: TextChunk[] = []
    const words = text.split(/\s+/)

    let currentChunk = ''
    let currentTokens = 0
    let start = 0
    let index = 0

    for (const word of words) {
      const wordTokens = this.countTokens(word + ' ')

      if (currentTokens + wordTokens <= chunkSize) {
        currentChunk += (currentChunk ? ' ' : '') + word
        currentTokens += wordTokens
      } else {
        if (currentChunk.trim()) {
          chunks.push({
            content: currentChunk.trim(),
            index,
            start,
            end: start + currentChunk.length,
            tokenCount: currentTokens,
          })
          index++
        }

        currentChunk = word
        currentTokens = wordTokens
        start += currentChunk.length + 1
      }
    }

    if (currentChunk.trim()) {
      chunks.push({
        content: currentChunk.trim(),
        index,
        start,
        end: start + currentChunk.length,
        tokenCount: currentTokens,
      })
    }

    return chunks
  }

  /**
   * –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ (–∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
   */
  static cleanup() {
    // –ù–µ—Ç —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –≤ fallback –≤–µ—Ä—Å–∏–∏
  }
}
