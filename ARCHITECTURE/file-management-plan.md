# üóÉÔ∏è File Management Rework Plan

## üéØ –¶–µ–ª—å: –£–º–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å —Ñ–∞–π–ª–∞–º–∏ –±–µ–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è

–ü–µ—Ä–µ–π—Ç–∏ –æ—Ç —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –∫ —Å–∏—Å—Ç–µ–º–µ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —É–¥–∞–ª–µ–Ω–∏–µ–º –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏.

## üîç –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ

### **–ß—Ç–æ —Å–µ–π—á–∞—Å –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:**
```
üìÅ Upload ‚Üí üíæ Store locally ‚Üí üîÑ Process ‚Üí üóÑÔ∏è Keep forever
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
- ‚ùå –§–∞–π–ª—ã –Ω–∞–∫–∞–ø–ª–∏–≤–∞—é—Ç—Å—è –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ (`uploads/` —Ä–∞—Å—Ç–µ—Ç)
- ‚ùå –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ: –æ–¥–∏–Ω —Ñ–∞–π–ª –º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–Ω–æ–≥–æ —Ä–∞–∑
- ‚ùå –¢—Ä–∞—Ç–∞ –¥–∏—Å–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
- ‚ùå –°–ª–æ–∂–Ω–æ—Å—Ç—å –æ—á–∏—Å—Ç–∫–∏ –∏ maintenance

### **–¢–µ–∫—É—â–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**
```
uploads/
‚îú‚îÄ‚îÄ 2025/07/30/           # –ü–∞–ø–∫–∏ –ø–æ –¥–∞—Ç–∞–º
‚îÇ   ‚îú‚îÄ‚îÄ file1.pdf
‚îÇ   ‚îî‚îÄ‚îÄ file2.pdf
‚îú‚îÄ‚îÄ temp/processing/      # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
‚îÇ   ‚îî‚îÄ‚îÄ [uuid]/
‚îî‚îÄ‚îÄ backups/             # –†–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏
```

## üéØ –ù–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞: Hash-Based Deduplication

### **–ù–æ–≤—ã–π flow:**
```
üìÅ Upload ‚Üí üîê Hash ‚Üí ‚ùì Exists? ‚Üí üîÑ Process ‚Üí üóëÔ∏è Delete ‚Üí üíæ Store hash only
```

### **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ù—É–ª–µ–≤–æ–µ –¥–∏—Å–∫–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è —Ñ–∞–π–ª–æ–≤
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
- ‚úÖ –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
- ‚úÖ –ü—Ä–æ—Å—Ç–æ—Ç–∞ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è

## üìã –ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### **Phase 1: –°—Ö–µ–º–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (1 –¥–µ–Ω—å)**

#### 1.1 –ù–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö–µ—à–µ–π —Ñ–∞–π–ª–æ–≤
```sql
-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
CREATE TABLE processed_files (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  file_hash TEXT UNIQUE NOT NULL,           -- SHA-256 —Ö–µ—à —Ñ–∞–π–ª–∞
  original_filename TEXT NOT NULL,          -- –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
  file_size INTEGER NOT NULL,               -- –†–∞–∑–º–µ—Ä –≤ –±–∞–π—Ç–∞—Ö
  mime_type TEXT NOT NULL,                  -- MIME —Ç–∏–ø —Ñ–∞–π–ª–∞
  processing_status TEXT DEFAULT 'pending', -- 'pending', 'processing', 'completed', 'failed'
  
  -- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
  chunks_created INTEGER DEFAULT 0,         -- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö chunks
  embeddings_created INTEGER DEFAULT 0,     -- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
  processing_time_ms INTEGER,               -- –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
  error_message TEXT,                       -- –°–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ, –µ—Å–ª–∏ –µ—Å—Ç—å
  
  -- –ê—É–¥–∏—Ç
  uploaded_by INTEGER REFERENCES users(id), -- –ö—Ç–æ –∑–∞–≥—Ä—É–∑–∏–ª
  uploaded_at DATETIME DEFAULT CURRENT_TIMESTAMP,
  processed_at DATETIME,                    -- –ö–æ–≥–¥–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞
  
  -- –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
  metadata_json TEXT,                       -- JSON —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
  
  UNIQUE(file_hash)
);

-- –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE INDEX idx_processed_files_hash ON processed_files(file_hash);
CREATE INDEX idx_processed_files_status ON processed_files(processing_status);
CREATE INDEX idx_processed_files_uploaded_by ON processed_files(uploaded_by);
```

#### 1.2 –°–≤—è–∑–∞–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è chunks
```sql
-- –°–≤—è–∑—å –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏ –∏ –∏—Ö chunks –≤ Qdrant
CREATE TABLE file_chunks (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  processed_file_id INTEGER REFERENCES processed_files(id) ON DELETE CASCADE,
  qdrant_point_id TEXT NOT NULL,            -- ID —Ç–æ—á–∫–∏ –≤ Qdrant
  chunk_index INTEGER NOT NULL,             -- –ü–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä chunk'–∞ –≤ —Ñ–∞–π–ª–µ
  chunk_text TEXT NOT NULL,                 -- –¢–µ–∫—Å—Ç chunk'–∞
  chunk_size INTEGER NOT NULL,              -- –†–∞–∑–º–µ—Ä chunk'–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
  created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
  
  UNIQUE(processed_file_id, chunk_index)
);

CREATE INDEX idx_file_chunks_file_id ON file_chunks(processed_file_id);
CREATE INDEX idx_file_chunks_qdrant_id ON file_chunks(qdrant_point_id);
```

### **Phase 2: –ù–æ–≤–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ (2-3 –¥–Ω—è)**

#### 2.1 –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
```typescript
// src/lib/file-hash.ts
import crypto from 'crypto'

export function calculateFileHash(buffer: Buffer): string {
  return crypto.createHash('sha256').update(buffer).digest('hex')
}

export async function checkFileExists(hash: string): Promise<boolean> {
  const db = await database
  const result = await db.get(
    'SELECT id FROM processed_files WHERE file_hash = ?',
    [hash]
  )
  return !!result
}

export async function getFileByHash(hash: string) {
  const db = await database
  return await db.get(`
    SELECT * FROM processed_files 
    WHERE file_hash = ? AND processing_status = 'completed'
  `, [hash])
}
```

#### 2.2 –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ñ–∞–π–ª–æ–≤
```typescript
// src/lib/file-processor-v2.ts
import { calculateFileHash, checkFileExists } from './file-hash'

export async function processUploadedFile(
  file: File, 
  uploadedBy: number
): Promise<{ 
  isDuplicate: boolean, 
  fileHash: string, 
  processedFileId?: number 
}> {
  
  // 1. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ buffer –∏ —Å—á–∏—Ç–∞–µ–º —Ö–µ—à
  const buffer = Buffer.from(await file.arrayBuffer())
  const fileHash = calculateFileHash(buffer)
  
  // 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª—Å—è –ª–∏ —É–∂–µ —ç—Ç–æ—Ç —Ñ–∞–π–ª
  const existingFile = await getFileByHash(fileHash)
  if (existingFile) {
    return {
      isDuplicate: true,
      fileHash,
      processedFileId: existingFile.id
    }
  }
  
  // 3. –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª –≤ –ë–î
  const db = await database
  const result = await db.run(`
    INSERT INTO processed_files (
      file_hash, original_filename, file_size, mime_type, 
      processing_status, uploaded_by
    ) VALUES (?, ?, ?, ?, 'processing', ?)
  `, [fileHash, file.name, buffer.length, file.type, uploadedBy])
  
  const processedFileId = result.lastID as number
  
  try {
    // 4. –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    const tempDir = path.join(process.cwd(), 'temp', 'processing', crypto.randomUUID())
    await fs.mkdir(tempDir, { recursive: true })
    const tempFilePath = path.join(tempDir, file.name)
    await fs.writeFile(tempFilePath, buffer)
    
    // 5. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª (–∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç, —Å–æ–∑–¥–∞–µ–º chunks)
    const startTime = Date.now()
    const chunks = await extractAndChunkText(tempFilePath)
    
    // 6. –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
    const embedResults = await createEmbeddingsForChunks(chunks, processedFileId)
    
    const processingTime = Date.now() - startTime
    
    // 7. –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≤ –ë–î
    await db.run(`
      UPDATE processed_files 
      SET processing_status = 'completed',
          chunks_created = ?,
          embeddings_created = ?,
          processing_time_ms = ?,
          processed_at = CURRENT_TIMESTAMP
      WHERE id = ?
    `, [chunks.length, embedResults.length, processingTime, processedFileId])
    
    // 8. üóëÔ∏è –£–î–ê–õ–Ø–ï–ú –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –∏ –ø–∞–ø–∫—É
    await fs.rm(tempDir, { recursive: true, force: true })
    
    return {
      isDuplicate: false,
      fileHash,
      processedFileId
    }
    
  } catch (error) {
    // –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ - –æ—Ç–º–µ—á–∞–µ–º –∫–∞–∫ failed –∏ —Ç–∞–∫–∂–µ —É–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    await db.run(`
      UPDATE processed_files 
      SET processing_status = 'failed',
          error_message = ?,
          processed_at = CURRENT_TIMESTAMP
      WHERE id = ?
    `, [error.message, processedFileId])
    
    // –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
    try {
      await fs.rm(tempDir, { recursive: true, force: true })
    } catch (cleanupError) {
      console.warn('Failed to cleanup temp files:', cleanupError)
    }
    
    throw error
  }
}

async function createEmbeddingsForChunks(chunks: string[], fileId: number) {
  const results = []
  
  for (let i = 0; i < chunks.length; i++) {
    const chunk = chunks[i]
    
    // –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
    const embedding = await getEmbedding(chunk)
    
    // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
    const pointId = `${fileId}_chunk_${i}`
    await upsertPoints([{
      id: pointId,
      vector: embedding,
      payload: {
        content: chunk,
        fileId,
        chunkIndex: i,
        metadata: { /* –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ */ }
      }
    }])
    
    // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤—è–∑—å –≤ –ë–î
    await database.run(`
      INSERT INTO file_chunks (
        processed_file_id, qdrant_point_id, chunk_index, 
        chunk_text, chunk_size
      ) VALUES (?, ?, ?, ?, ?)
    `, [fileId, pointId, i, chunk, chunk.length])
    
    results.push({ pointId, chunkIndex: i })
  }
  
  return results
}
```

### **Phase 3: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ API endpoints (1-2 –¥–Ω—è)**

#### 3.1 –ù–æ–≤—ã–π `/api/upload/route.ts`
```typescript
export async function POST(request: Request) {
  // ... –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è ...
  
  const formData = await request.formData()
  const file = formData.get('file') as File
  
  if (!file) {
    return NextResponse.json({ error: 'No file provided' }, { status: 400 })
  }
  
  try {
    const result = await processUploadedFile(file, userId)
    
    if (result.isDuplicate) {
      return NextResponse.json({
        success: true,
        message: 'File already exists and has been processed',
        isDuplicate: true,
        fileHash: result.fileHash,
        processedFileId: result.processedFileId
      })
    }
    
    return NextResponse.json({
      success: true,
      message: 'File processed successfully',
      isDuplicate: false,
      fileHash: result.fileHash,
      processedFileId: result.processedFileId
    })
    
  } catch (error) {
    return NextResponse.json(
      { error: 'File processing failed', details: error.message },
      { status: 500 }
    )
  }
}
```

#### 3.2 –ù–æ–≤—ã–π endpoint –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ñ–∞–π–ª–æ–≤
```typescript
// src/app/api/admin/files/stats/route.ts
export async function GET() {
  const db = await database
  
  const stats = await db.get(`
    SELECT 
      COUNT(*) as total_files,
      COUNT(CASE WHEN processing_status = 'completed' THEN 1 END) as completed_files,
      COUNT(CASE WHEN processing_status = 'failed' THEN 1 END) as failed_files,
      COUNT(CASE WHEN processing_status = 'processing' THEN 1 END) as processing_files,
      SUM(file_size) as total_size_bytes,
      SUM(chunks_created) as total_chunks,
      AVG(processing_time_ms) as avg_processing_time_ms
    FROM processed_files
  `)
  
  return NextResponse.json({
    ...stats,
    total_size_mb: Math.round(stats.total_size_bytes / 1024 / 1024 * 100) / 100
  })
}
```

### **Phase 4: –ú–∏–≥—Ä–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ (1-2 –¥–Ω—è)**

#### 4.1 –°–∫—Ä–∏–ø—Ç –º–∏–≥—Ä–∞—Ü–∏–∏
```typescript
// scripts/migrate-existing-files.ts
export async function migrateExistingFiles() {
  const uploadsDir = path.join(process.cwd(), 'uploads')
  const existingFiles = await findAllPdfFiles(uploadsDir)
  
  console.log(`Found ${existingFiles.length} files to migrate`)
  
  let migratedCount = 0
  let skippedCount = 0
  let errorCount = 0
  
  for (const filePath of existingFiles) {
    try {
      const buffer = await fs.readFile(filePath)
      const fileHash = calculateFileHash(buffer)
      
      // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –º–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω –ª–∏ —É–∂–µ
      if (await checkFileExists(fileHash)) {
        console.log(`Skipping ${filePath} - already exists`)
        skippedCount++
        continue
      }
      
      // –ú–∏–≥—Ä–∏—Ä—É–µ–º —Ñ–∞–π–ª
      await migrateFile(filePath, buffer, fileHash)
      migratedCount++
      
    } catch (error) {
      console.error(`Error migrating ${filePath}:`, error)
      errorCount++
    }
  }
  
  console.log(`Migration completed: ${migratedCount} migrated, ${skippedCount} skipped, ${errorCount} errors`)
  
  // –ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏ –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –ø–∞–ø–∫—É uploads
  if (errorCount === 0) {
    console.log('All files migrated successfully. Consider removing uploads/ directory.')
  }
}
```

### **Phase 5: Cleanup –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ (1 –¥–µ–Ω—å)**

#### 5.1 –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
```typescript
// src/lib/cleanup.ts
export async function cleanupTempFiles() {
  const tempDir = path.join(process.cwd(), 'temp', 'processing')
  
  try {
    const entries = await fs.readdir(tempDir, { withFileTypes: true })
    
    for (const entry of entries) {
      if (entry.isDirectory()) {
        const dirPath = path.join(tempDir, entry.name)
        const stats = await fs.stat(dirPath)
        
        // –£–¥–∞–ª—è–µ–º –ø–∞–ø–∫–∏ —Å—Ç–∞—Ä—à–µ 1 —á–∞—Å–∞
        const hourAgo = Date.now() - (60 * 60 * 1000)
        if (stats.mtime.getTime() < hourAgo) {
          await fs.rm(dirPath, { recursive: true, force: true })
          console.log(`Cleaned up old temp directory: ${entry.name}`)
        }
      }
    }
  } catch (error) {
    console.error('Cleanup error:', error)
  }
}

// –ó–∞–ø—É—Å–∫–∞—Ç—å –∫–∞–∂–¥—ã–π —á–∞—Å
setInterval(cleanupTempFiles, 60 * 60 * 1000)
```

#### 5.2 Dashboard –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
```typescript
// src/app/admin/files/page.tsx - –∞–¥–º–∏–Ω-—Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
export default async function FilesAdminPage() {
  const stats = await fetch('/api/admin/files/stats').then(r => r.json())
  
  return (
    <div className="space-y-6">
      <h1>File Management Dashboard</h1>
      
      <div className="grid grid-cols-4 gap-4">
        <StatCard title="Total Files" value={stats.total_files} />
        <StatCard title="Total Size" value={`${stats.total_size_mb} MB`} />
        <StatCard title="Total Chunks" value={stats.total_chunks} />
        <StatCard title="Avg Processing Time" value={`${Math.round(stats.avg_processing_time_ms)}ms`} />
      </div>
      
      {/* –¢–∞–±–ª–∏—Ü–∞ —Å —Ñ–∞–π–ª–∞–º–∏, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫ –∏ —Ç.–¥. */}
    </div>
  )
}
```

## üìä –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

### **–≠–∫–æ–Ω–æ–º–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤:**
- üìâ **-100% –¥–∏—Å–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞** –¥–ª—è —Ñ–∞–π–ª–æ–≤ (—Ç–æ–ª—å–∫–æ –ë–î)
- üìâ **-90% –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é** (instant hash lookup)
- üìâ **-80% —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ maintenance** (no file cleanup needed)

### **–£–ª—É—á—à–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã:**
- ‚ö° **–ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤** —á–µ—Ä–µ–∑ —Ö–µ—à
- üìä **–î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞** –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤  
- üîÑ **–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å** - –Ω–µ—Ç "–æ—Å–∏—Ä–æ—Ç–µ–≤—à–∏—Ö" —Ñ–∞–π–ª–æ–≤
- üóÑÔ∏è **–ê—É–¥–∏—Ç** - –ø–æ–ª–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –∑–∞–≥—Ä—É–∑–æ–∫

### **–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å:**
- üîê **–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ —Ö–µ—à—É** - –æ–¥–∏–Ω —Ñ–∞–π–ª = –æ–¥–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞
- üßπ **–ê–≤—Ç–æ–æ—á–∏—Å—Ç–∫–∞** –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
- üìù **–ê—É–¥–∏—Ç –ª–æ–≥–∏** –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π

## ‚úÖ –ö—Ä–∏—Ç–µ—Ä–∏–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏

- [ ] –í—Å–µ –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –ø–æ –Ω–æ–≤–æ–π —Å—Ö–µ–º–µ
- [ ] –î—É–±–ª–∏–∫–∞—Ç—ã –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –ø–æ —Ö–µ—à—É –∑–∞ < 100ms
- [ ] –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —É–¥–∞–ª—è—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
- [ ] –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ –∞–¥–º–∏–Ω-–ø–∞–Ω–µ–ª–∏
- [ ] –ú–∏–≥—Ä–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞
- [ ] –ü–∞–ø–∫–∞ `uploads/` –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è

---
*Smart file management - foundation –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–π —Å–∏—Å—Ç–µ–º—ã.*